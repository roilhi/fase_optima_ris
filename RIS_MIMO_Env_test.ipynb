{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RIS_MIMO_environment import RIS_MIMO_env\n",
    "# import helpers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "# Import stable baselines stuff\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = {\"ntx_antennas\":8,\n",
    "        \"nrx_antennas\":2,\n",
    "        \"n_users\":4,\n",
    "        \"nris_surfaces\":32,\n",
    "        \"nris_elements\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RIS_MIMO_env(**env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1,episodes+1):\n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done ,info = env.step(action)\n",
    "    score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "action_noise = NormalActionNoise(mean=np.zeros(env.action_space.shape), sigma=0.1 * np.ones(env.action_space.shape))\n",
    "model = DDPG(\n",
    "    policy='MlpPolicy', \n",
    "    env=env,\n",
    "    buffer_size=100000,\n",
    "    learning_rate= 0.001,\n",
    "    batch_size=16,\n",
    "    tau=0.001,\n",
    "    action_noise=action_noise,\n",
    "    verbose=1)\n",
    "model.learn(total_timesteps=1000)\n",
    "model_name = 'ppoRISMIMO-32e-1ris-4u'\n",
    "ris_miso_models = os.path.join('RIS_MIMO_models',model_name)\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = Monitor(env)\n",
    "#model = PPO.load(shower_path,env)\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward = {mean_reward:.2f} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
